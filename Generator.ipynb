{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc98a5b5",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4eac42",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad48f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # print(file)\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59f42ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import io\n",
    "\n",
    "#with io.open(filename, encoding='utf-8', mode='r') as file:\n",
    "#    for line in file:\n",
    "#        process(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f317288e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿BOOK I.\n",
      "\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in wh\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'republic_clean.txt'\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d984b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text\n",
    "#import string\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39213c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said', 'to', 'me']\n",
      "Total Tokens: 117341\n",
      "Unique Tokens: 7323\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26a463f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 117290\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd1e128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, republic_sequences):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(republic_sequences, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8669f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'republic_sequences.txt'\n",
    "save_doc(sequences, out_filename)\n",
    "doc_sequences = load_doc(out_filename)\n",
    "\n",
    "#i = 1\n",
    "#with open('republic_sequences.txt', 'r') as f:\n",
    "#    for lines in f:\n",
    "#        if i >= 500:\n",
    "#            break\n",
    "#        else:\n",
    "#            print(doc_sequences[i:51])\n",
    "#            i = i + 51\n",
    "\n",
    "#with open('republic_sequences.txt', 'r') as f:\n",
    "#    for line in f:\n",
    "#        print(doc_sequences[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c8d99",
   "metadata": {},
   "source": [
    "### Train Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd9bec",
   "metadata": {},
   "source": [
    " \n",
    "    We will use an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3468a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c87c2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d3e3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a95804a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca9864",
   "metadata": {},
   "source": [
    "Обученное встраивание должно знать размер словаря и длину входных последовательностей, как обсуждалось ранее. У него также есть параметр, указывающий, сколько измерений будет использоваться для представления каждого слова. То есть размер векторного пространства вложения.\n",
    "\n",
    "Распространенными значениями являются 50, 100 и 300. Мы будем использовать здесь 50, но рассмотрите возможность тестирования меньших или больших значений.\n",
    "\n",
    "Мы будем использовать два скрытых слоя LSTM по 100 ячеек памяти каждый. Больше ячеек памяти и более глубокая сеть могут дать лучшие результаты.\n",
    "\n",
    "Плотный полностью связанный слой со 100 нейронами соединяется со скрытыми слоями LSTM для интерпретации признаков, извлеченных из последовательности. Выходной слой предсказывает следующее слово как единый вектор размера словаря с вероятностью для каждого слова в словаре. Функция активации softmax используется для обеспечения того, чтобы выходные данные имели характеристики нормализованных вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22c19124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 50)            366200    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7324)              739724    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,256,824\n",
      "Trainable params: 1,256,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b2199",
   "metadata": {},
   "source": [
    "Затем модель компилируется с указанием категориальной потери перекрестной энтропии, необходимой для соответствия модели. Технически модель изучает многоклассовую классификацию, и это подходящая функция потерь для такого типа задач. Используется эффективная реализация Адама для мини-пакетного градиентного спуска, и оценивается точность модели.\n",
    "\n",
    "Наконец, модель соответствует данным для 100 обучающих эпох со скромным размером пакета 128 для ускорения процесса.\n",
    "\n",
    "Обучение может занять несколько часов на современном оборудовании без графических процессоров. Вы можете ускорить его, увеличив размер партии и/или уменьшив количество периодов обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194fcae",
   "metadata": {},
   "source": [
    "Во время обучения вы увидите сводку производительности, включая потери и точность, оцененные на основе обучающих данных в конце каждого пакетного обновления.\n",
    "\n",
    "##### Примечание. Ваши результаты могут отличаться из-за стохастического характера алгоритма или процедуры оценки, а также из-за различий в численной точности. Попробуйте запустить пример несколько раз и сравните средний результат.\n",
    "\n",
    "##### Вы получите разные результаты, но, возможно, точность предсказания следующего слова в последовательности составит чуть более 50 %, что неплохо. Мы не стремимся к 100% точности (например, модель, которая запоминает текст), а скорее к модели, которая улавливает суть текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4913b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "917/917 [==============================] - 102s 108ms/step - loss: 6.1362 - accuracy: 0.0751\n",
      "Epoch 2/100\n",
      "917/917 [==============================] - 96s 105ms/step - loss: 5.6759 - accuracy: 0.1094\n",
      "Epoch 3/100\n",
      "917/917 [==============================] - 97s 105ms/step - loss: 5.4402 - accuracy: 0.1331\n",
      "Epoch 4/100\n",
      "917/917 [==============================] - 96s 105ms/step - loss: 5.2810 - accuracy: 0.1466\n",
      "Epoch 5/100\n",
      "917/917 [==============================] - 96s 105ms/step - loss: 5.1669 - accuracy: 0.1554\n",
      "Epoch 6/100\n",
      "917/917 [==============================] - 97s 106ms/step - loss: 5.0691 - accuracy: 0.1630\n",
      "Epoch 7/100\n",
      "917/917 [==============================] - 96s 105ms/step - loss: 4.9802 - accuracy: 0.1689\n",
      "Epoch 8/100\n",
      "917/917 [==============================] - 97s 106ms/step - loss: 4.8978 - accuracy: 0.1739\n",
      "Epoch 9/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 4.8201 - accuracy: 0.1774\n",
      "Epoch 10/100\n",
      "917/917 [==============================] - 98s 107ms/step - loss: 4.7501 - accuracy: 0.1809\n",
      "Epoch 11/100\n",
      "917/917 [==============================] - 98s 107ms/step - loss: 4.6821 - accuracy: 0.1841\n",
      "Epoch 12/100\n",
      "917/917 [==============================] - 98s 107ms/step - loss: 4.6187 - accuracy: 0.1865\n",
      "Epoch 13/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.5603 - accuracy: 0.1889\n",
      "Epoch 14/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.5042 - accuracy: 0.1915\n",
      "Epoch 15/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.4503 - accuracy: 0.1945\n",
      "Epoch 16/100\n",
      "917/917 [==============================] - 98s 107ms/step - loss: 4.3978 - accuracy: 0.1967\n",
      "Epoch 17/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.3475 - accuracy: 0.1991\n",
      "Epoch 18/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 4.3007 - accuracy: 0.2020\n",
      "Epoch 19/100\n",
      "917/917 [==============================] - 98s 107ms/step - loss: 4.2558 - accuracy: 0.2042\n",
      "Epoch 20/100\n",
      "917/917 [==============================] - 98s 107ms/step - loss: 4.2143 - accuracy: 0.2068\n",
      "Epoch 21/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.1727 - accuracy: 0.2093\n",
      "Epoch 22/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.1335 - accuracy: 0.2124\n",
      "Epoch 23/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.0976 - accuracy: 0.2146\n",
      "Epoch 24/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 4.0627 - accuracy: 0.2168\n",
      "Epoch 25/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 4.0303 - accuracy: 0.2199\n",
      "Epoch 26/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 3.9986 - accuracy: 0.2224\n",
      "Epoch 27/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 3.9684 - accuracy: 0.2247\n",
      "Epoch 28/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 3.9401 - accuracy: 0.2279\n",
      "Epoch 29/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 3.9113 - accuracy: 0.2298\n",
      "Epoch 30/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 3.8852 - accuracy: 0.2334\n",
      "Epoch 31/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 3.8589 - accuracy: 0.2348\n",
      "Epoch 32/100\n",
      "917/917 [==============================] - 99s 108ms/step - loss: 3.8349 - accuracy: 0.2379\n",
      "Epoch 33/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 3.8075 - accuracy: 0.2405\n",
      "Epoch 34/100\n",
      "917/917 [==============================] - 101s 110ms/step - loss: 3.7840 - accuracy: 0.2430\n",
      "Epoch 35/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 3.7614 - accuracy: 0.2457\n",
      "Epoch 36/100\n",
      "917/917 [==============================] - 101s 110ms/step - loss: 3.7352 - accuracy: 0.2495\n",
      "Epoch 37/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 3.7137 - accuracy: 0.2499\n",
      "Epoch 38/100\n",
      "917/917 [==============================] - 100s 109ms/step - loss: 3.6925 - accuracy: 0.2528\n",
      "Epoch 39/100\n",
      "917/917 [==============================] - 101s 110ms/step - loss: 3.6701 - accuracy: 0.2556\n",
      "Epoch 40/100\n",
      "917/917 [==============================] - 100s 110ms/step - loss: 3.6498 - accuracy: 0.2591\n",
      "Epoch 41/100\n",
      "917/917 [==============================] - 101s 110ms/step - loss: 3.6280 - accuracy: 0.2608\n",
      "Epoch 42/100\n",
      "917/917 [==============================] - 103s 112ms/step - loss: 3.6049 - accuracy: 0.2640\n",
      "Epoch 43/100\n",
      "917/917 [==============================] - 102s 111ms/step - loss: 3.5847 - accuracy: 0.2669\n",
      "Epoch 44/100\n",
      "917/917 [==============================] - 103s 112ms/step - loss: 3.5653 - accuracy: 0.2684\n",
      "Epoch 45/100\n",
      "917/917 [==============================] - 104s 113ms/step - loss: 3.5439 - accuracy: 0.2721\n",
      "Epoch 46/100\n",
      "917/917 [==============================] - 103s 112ms/step - loss: 3.5234 - accuracy: 0.2752\n",
      "Epoch 47/100\n",
      "917/917 [==============================] - 106s 116ms/step - loss: 3.5067 - accuracy: 0.2759\n",
      "Epoch 48/100\n",
      "917/917 [==============================] - 108s 118ms/step - loss: 3.4853 - accuracy: 0.2791\n",
      "Epoch 49/100\n",
      "917/917 [==============================] - 103s 113ms/step - loss: 3.4677 - accuracy: 0.2814\n",
      "Epoch 50/100\n",
      "917/917 [==============================] - 103s 113ms/step - loss: 3.4493 - accuracy: 0.2836\n",
      "Epoch 51/100\n",
      "917/917 [==============================] - 105s 114ms/step - loss: 3.4321 - accuracy: 0.2872\n",
      "Epoch 52/100\n",
      "917/917 [==============================] - 104s 114ms/step - loss: 3.4117 - accuracy: 0.2894\n",
      "Epoch 53/100\n",
      "917/917 [==============================] - 105s 115ms/step - loss: 3.3946 - accuracy: 0.2904\n",
      "Epoch 54/100\n",
      "917/917 [==============================] - 105s 114ms/step - loss: 3.3756 - accuracy: 0.2938\n",
      "Epoch 55/100\n",
      "917/917 [==============================] - 104s 113ms/step - loss: 3.3579 - accuracy: 0.2955\n",
      "Epoch 56/100\n",
      "917/917 [==============================] - 104s 113ms/step - loss: 3.3420 - accuracy: 0.2989\n",
      "Epoch 57/100\n",
      "917/917 [==============================] - 104s 113ms/step - loss: 3.3228 - accuracy: 0.3016\n",
      "Epoch 58/100\n",
      "917/917 [==============================] - 104s 114ms/step - loss: 3.3067 - accuracy: 0.3042\n",
      "Epoch 59/100\n",
      "917/917 [==============================] - 105s 114ms/step - loss: 3.2896 - accuracy: 0.3061\n",
      "Epoch 60/100\n",
      "917/917 [==============================] - 105s 114ms/step - loss: 3.2716 - accuracy: 0.3085\n",
      "Epoch 61/100\n",
      "917/917 [==============================] - 104s 114ms/step - loss: 3.2549 - accuracy: 0.3106\n",
      "Epoch 62/100\n",
      "917/917 [==============================] - 105s 114ms/step - loss: 3.2403 - accuracy: 0.3134\n",
      "Epoch 63/100\n",
      "917/917 [==============================] - 106s 116ms/step - loss: 3.2242 - accuracy: 0.3162\n",
      "Epoch 64/100\n",
      "917/917 [==============================] - 107s 117ms/step - loss: 3.2068 - accuracy: 0.3177\n",
      "Epoch 65/100\n",
      "917/917 [==============================] - 109s 119ms/step - loss: 3.1909 - accuracy: 0.3203\n",
      "Epoch 66/100\n",
      "917/917 [==============================] - 108s 117ms/step - loss: 3.1728 - accuracy: 0.3240\n",
      "Epoch 67/100\n",
      "917/917 [==============================] - 108s 118ms/step - loss: 3.1608 - accuracy: 0.3253\n",
      "Epoch 68/100\n",
      "917/917 [==============================] - 109s 119ms/step - loss: 3.1441 - accuracy: 0.3290\n",
      "Epoch 69/100\n",
      "917/917 [==============================] - 110s 119ms/step - loss: 3.1262 - accuracy: 0.3316\n",
      "Epoch 70/100\n",
      "917/917 [==============================] - 109s 119ms/step - loss: 3.1139 - accuracy: 0.3327\n",
      "Epoch 71/100\n",
      "917/917 [==============================] - 107s 116ms/step - loss: 3.0968 - accuracy: 0.3353\n",
      "Epoch 72/100\n",
      "917/917 [==============================] - 107s 117ms/step - loss: 3.0823 - accuracy: 0.3382\n",
      "Epoch 73/100\n",
      "917/917 [==============================] - 107s 117ms/step - loss: 3.0665 - accuracy: 0.3397\n",
      "Epoch 74/100\n",
      "917/917 [==============================] - 109s 118ms/step - loss: 3.0534 - accuracy: 0.3422\n",
      "Epoch 75/100\n",
      "917/917 [==============================] - 108s 118ms/step - loss: 3.0355 - accuracy: 0.3464\n",
      "Epoch 76/100\n",
      "917/917 [==============================] - 109s 119ms/step - loss: 3.0218 - accuracy: 0.3484\n",
      "Epoch 77/100\n",
      "917/917 [==============================] - 110s 120ms/step - loss: 3.0074 - accuracy: 0.3507\n",
      "Epoch 78/100\n",
      "917/917 [==============================] - 110s 120ms/step - loss: 2.9942 - accuracy: 0.3522\n",
      "Epoch 79/100\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.9774 - accuracy: 0.3543\n",
      "Epoch 80/100\n",
      "917/917 [==============================] - 110s 120ms/step - loss: 2.9633 - accuracy: 0.3576\n",
      "Epoch 81/100\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.9498 - accuracy: 0.3593\n",
      "Epoch 82/100\n",
      "917/917 [==============================] - 111s 122ms/step - loss: 2.9343 - accuracy: 0.3608\n",
      "Epoch 83/100\n",
      "917/917 [==============================] - 110s 120ms/step - loss: 2.9211 - accuracy: 0.3645\n",
      "Epoch 84/100\n",
      "917/917 [==============================] - 110s 120ms/step - loss: 2.9047 - accuracy: 0.3663\n",
      "Epoch 85/100\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.8913 - accuracy: 0.3695\n",
      "Epoch 86/100\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.8791 - accuracy: 0.3711\n",
      "Epoch 87/100\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.8661 - accuracy: 0.3730\n",
      "Epoch 88/100\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.8537 - accuracy: 0.3754\n",
      "Epoch 89/100\n",
      "917/917 [==============================] - 112s 122ms/step - loss: 2.8397 - accuracy: 0.3787\n",
      "Epoch 90/100\n",
      "917/917 [==============================] - 114s 124ms/step - loss: 2.8270 - accuracy: 0.3801\n",
      "Epoch 91/100\n",
      "917/917 [==============================] - 114s 124ms/step - loss: 2.8099 - accuracy: 0.3841\n",
      "Epoch 92/100\n",
      "917/917 [==============================] - 116s 127ms/step - loss: 2.7995 - accuracy: 0.3859\n",
      "Epoch 93/100\n",
      "917/917 [==============================] - 116s 127ms/step - loss: 2.7850 - accuracy: 0.3875\n",
      "Epoch 94/100\n",
      "917/917 [==============================] - 117s 128ms/step - loss: 2.7721 - accuracy: 0.3902\n",
      "Epoch 95/100\n",
      "917/917 [==============================] - 117s 127ms/step - loss: 2.7595 - accuracy: 0.3917\n",
      "Epoch 96/100\n",
      "917/917 [==============================] - 115s 126ms/step - loss: 2.7445 - accuracy: 0.3955\n",
      "Epoch 97/100\n",
      "917/917 [==============================] - 113s 123ms/step - loss: 2.7357 - accuracy: 0.3962\n",
      "Epoch 98/100\n",
      "917/917 [==============================] - 115s 125ms/step - loss: 2.7225 - accuracy: 0.3986\n",
      "Epoch 99/100\n",
      "917/917 [==============================] - 115s 126ms/step - loss: 2.7087 - accuracy: 0.4008\n",
      "Epoch 100/100\n",
      "917/917 [==============================] - 115s 125ms/step - loss: 2.6955 - accuracy: 0.4033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b1b7f6160>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc921689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2121011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "917/917 [==============================] - 121s 124ms/step - loss: 2.7794 - accuracy: 0.3885\n",
      "Epoch 2/250\n",
      "917/917 [==============================] - 106s 116ms/step - loss: 2.6854 - accuracy: 0.4049\n",
      "Epoch 3/250\n",
      "917/917 [==============================] - 106s 116ms/step - loss: 2.6693 - accuracy: 0.4089\n",
      "Epoch 4/250\n",
      "917/917 [==============================] - 106s 115ms/step - loss: 2.6540 - accuracy: 0.4102\n",
      "Epoch 5/250\n",
      "917/917 [==============================] - 106s 115ms/step - loss: 2.6385 - accuracy: 0.4142\n",
      "Epoch 6/250\n",
      "917/917 [==============================] - 105s 115ms/step - loss: 2.6278 - accuracy: 0.4152\n",
      "Epoch 7/250\n",
      "917/917 [==============================] - 105s 114ms/step - loss: 2.6148 - accuracy: 0.4175\n",
      "Epoch 8/250\n",
      "917/917 [==============================] - 105s 114ms/step - loss: 2.6013 - accuracy: 0.4215\n",
      "Epoch 9/250\n",
      "917/917 [==============================] - 106s 115ms/step - loss: 2.5895 - accuracy: 0.4216\n",
      "Epoch 10/250\n",
      "917/917 [==============================] - 106s 116ms/step - loss: 2.5775 - accuracy: 0.4246\n",
      "Epoch 11/250\n",
      "917/917 [==============================] - 107s 117ms/step - loss: 2.5646 - accuracy: 0.4264\n",
      "Epoch 12/250\n",
      "917/917 [==============================] - 109s 119ms/step - loss: 2.5529 - accuracy: 0.4289\n",
      "Epoch 13/250\n",
      "917/917 [==============================] - 107s 117ms/step - loss: 2.5444 - accuracy: 0.4296\n",
      "Epoch 14/250\n",
      "917/917 [==============================] - 108s 118ms/step - loss: 2.5315 - accuracy: 0.4333\n",
      "Epoch 15/250\n",
      "917/917 [==============================] - 107s 117ms/step - loss: 2.5185 - accuracy: 0.4353\n",
      "Epoch 16/250\n",
      "917/917 [==============================] - 110s 120ms/step - loss: 2.5109 - accuracy: 0.4367\n",
      "Epoch 17/250\n",
      "917/917 [==============================] - 110s 120ms/step - loss: 2.4957 - accuracy: 0.4395\n",
      "Epoch 18/250\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.4870 - accuracy: 0.4404\n",
      "Epoch 19/250\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.4735 - accuracy: 0.4430\n",
      "Epoch 20/250\n",
      "917/917 [==============================] - 111s 122ms/step - loss: 2.4658 - accuracy: 0.4445\n",
      "Epoch 21/250\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.4536 - accuracy: 0.4470\n",
      "Epoch 22/250\n",
      "917/917 [==============================] - 112s 122ms/step - loss: 2.4412 - accuracy: 0.4499\n",
      "Epoch 23/250\n",
      "917/917 [==============================] - 111s 121ms/step - loss: 2.4338 - accuracy: 0.4503\n",
      "Epoch 24/250\n",
      "917/917 [==============================] - 113s 124ms/step - loss: 2.4213 - accuracy: 0.4523\n",
      "Epoch 25/250\n",
      "917/917 [==============================] - 113s 123ms/step - loss: 2.4122 - accuracy: 0.4535\n",
      "Epoch 26/250\n",
      "917/917 [==============================] - 113s 123ms/step - loss: 2.4016 - accuracy: 0.4584\n",
      "Epoch 27/250\n",
      "917/917 [==============================] - 113s 124ms/step - loss: 2.3896 - accuracy: 0.4594\n",
      "Epoch 28/250\n",
      "917/917 [==============================] - 114s 124ms/step - loss: 2.3815 - accuracy: 0.4585\n",
      "Epoch 29/250\n",
      "917/917 [==============================] - 115s 125ms/step - loss: 2.3702 - accuracy: 0.4618\n",
      "Epoch 30/250\n",
      "917/917 [==============================] - 114s 125ms/step - loss: 2.3613 - accuracy: 0.4648\n",
      "Epoch 31/250\n",
      "917/917 [==============================] - 114s 124ms/step - loss: 2.3485 - accuracy: 0.4658\n",
      "Epoch 32/250\n",
      "917/917 [==============================] - 116s 126ms/step - loss: 2.3414 - accuracy: 0.4693\n",
      "Epoch 33/250\n",
      "917/917 [==============================] - 116s 127ms/step - loss: 2.3309 - accuracy: 0.4715\n",
      "Epoch 34/250\n",
      "917/917 [==============================] - 117s 128ms/step - loss: 2.3212 - accuracy: 0.4725\n",
      "Epoch 35/250\n",
      "917/917 [==============================] - 118s 128ms/step - loss: 2.3146 - accuracy: 0.4738\n",
      "Epoch 36/250\n",
      "917/917 [==============================] - 118s 129ms/step - loss: 2.2990 - accuracy: 0.4768\n",
      "Epoch 37/250\n",
      "917/917 [==============================] - 119s 130ms/step - loss: 2.2924 - accuracy: 0.4769\n",
      "Epoch 38/250\n",
      "917/917 [==============================] - 119s 130ms/step - loss: 2.2844 - accuracy: 0.4794\n",
      "Epoch 39/250\n",
      "917/917 [==============================] - 119s 130ms/step - loss: 2.2774 - accuracy: 0.4802\n",
      "Epoch 40/250\n",
      "917/917 [==============================] - 118s 129ms/step - loss: 2.2660 - accuracy: 0.4823\n",
      "Epoch 41/250\n",
      "917/917 [==============================] - 118s 129ms/step - loss: 2.2561 - accuracy: 0.4848\n",
      "Epoch 42/250\n",
      "917/917 [==============================] - 121s 131ms/step - loss: 2.2458 - accuracy: 0.4868\n",
      "Epoch 43/250\n",
      "917/917 [==============================] - 121s 132ms/step - loss: 2.2439 - accuracy: 0.4861\n",
      "Epoch 44/250\n",
      "917/917 [==============================] - 121s 132ms/step - loss: 2.2298 - accuracy: 0.4902\n",
      "Epoch 45/250\n",
      "917/917 [==============================] - 122s 133ms/step - loss: 2.2171 - accuracy: 0.4920\n",
      "Epoch 46/250\n",
      "917/917 [==============================] - 123s 134ms/step - loss: 2.2109 - accuracy: 0.4923\n",
      "Epoch 47/250\n",
      "917/917 [==============================] - 123s 134ms/step - loss: 2.2002 - accuracy: 0.4964\n",
      "Epoch 48/250\n",
      "917/917 [==============================] - 122s 133ms/step - loss: 2.1979 - accuracy: 0.4959\n",
      "Epoch 49/250\n",
      "917/917 [==============================] - 123s 134ms/step - loss: 2.1934 - accuracy: 0.4973\n",
      "Epoch 50/250\n",
      "917/917 [==============================] - 125s 136ms/step - loss: 2.1710 - accuracy: 0.5027\n",
      "Epoch 51/250\n",
      "917/917 [==============================] - 126s 137ms/step - loss: 2.1679 - accuracy: 0.5028\n",
      "Epoch 52/250\n",
      "917/917 [==============================] - 127s 138ms/step - loss: 2.1632 - accuracy: 0.5023\n",
      "Epoch 53/250\n",
      "917/917 [==============================] - 127s 138ms/step - loss: 2.1565 - accuracy: 0.5039\n",
      "Epoch 54/250\n",
      "917/917 [==============================] - 128s 139ms/step - loss: 2.1472 - accuracy: 0.5052\n",
      "Epoch 55/250\n",
      "917/917 [==============================] - 127s 138ms/step - loss: 2.1384 - accuracy: 0.5078\n",
      "Epoch 56/250\n",
      "917/917 [==============================] - 129s 140ms/step - loss: 2.1275 - accuracy: 0.5096\n",
      "Epoch 57/250\n",
      "917/917 [==============================] - 130s 141ms/step - loss: 2.1179 - accuracy: 0.5109\n",
      "Epoch 58/250\n",
      "917/917 [==============================] - 131s 143ms/step - loss: 2.1085 - accuracy: 0.5130\n",
      "Epoch 59/250\n",
      "917/917 [==============================] - 131s 143ms/step - loss: 2.1039 - accuracy: 0.5152\n",
      "Epoch 60/250\n",
      "917/917 [==============================] - 132s 144ms/step - loss: 2.0980 - accuracy: 0.5159\n",
      "Epoch 61/250\n",
      "917/917 [==============================] - 133s 144ms/step - loss: 2.0896 - accuracy: 0.5173\n",
      "Epoch 62/250\n",
      "917/917 [==============================] - 130s 142ms/step - loss: 2.0898 - accuracy: 0.5160\n",
      "Epoch 63/250\n",
      "917/917 [==============================] - 134s 146ms/step - loss: 2.0698 - accuracy: 0.5207\n",
      "Epoch 64/250\n",
      "917/917 [==============================] - 147s 160ms/step - loss: 2.0657 - accuracy: 0.5225\n",
      "Epoch 65/250\n",
      "917/917 [==============================] - 148s 162ms/step - loss: 2.0516 - accuracy: 0.5255\n",
      "Epoch 66/250\n",
      "917/917 [==============================] - 148s 162ms/step - loss: 2.0515 - accuracy: 0.5251\n",
      "Epoch 67/250\n",
      "917/917 [==============================] - 149s 162ms/step - loss: 2.0407 - accuracy: 0.5268\n",
      "Epoch 68/250\n",
      "917/917 [==============================] - 148s 162ms/step - loss: 2.0363 - accuracy: 0.5273\n",
      "Epoch 69/250\n",
      "917/917 [==============================] - 149s 162ms/step - loss: 2.0269 - accuracy: 0.5294\n",
      "Epoch 70/250\n",
      "917/917 [==============================] - 152s 166ms/step - loss: 2.0188 - accuracy: 0.5313\n",
      "Epoch 71/250\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 2.0114 - accuracy: 0.5335\n",
      "Epoch 72/250\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 2.0178 - accuracy: 0.5312\n",
      "Epoch 73/250\n",
      "917/917 [==============================] - 155s 169ms/step - loss: 1.9974 - accuracy: 0.5353\n",
      "Epoch 74/250\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 1.9922 - accuracy: 0.5367\n",
      "Epoch 75/250\n",
      "917/917 [==============================] - 153s 167ms/step - loss: 1.9829 - accuracy: 0.5383\n",
      "Epoch 76/250\n",
      "917/917 [==============================] - 157s 171ms/step - loss: 1.9766 - accuracy: 0.5396\n",
      "Epoch 77/250\n",
      "917/917 [==============================] - 155s 170ms/step - loss: 1.9744 - accuracy: 0.5410\n",
      "Epoch 78/250\n",
      "917/917 [==============================] - 155s 169ms/step - loss: 1.9739 - accuracy: 0.5393\n",
      "Epoch 79/250\n",
      "917/917 [==============================] - 156s 170ms/step - loss: 1.9523 - accuracy: 0.5455\n",
      "Epoch 80/250\n",
      "917/917 [==============================] - 155s 169ms/step - loss: 1.9477 - accuracy: 0.5465\n",
      "Epoch 81/250\n",
      "917/917 [==============================] - 156s 170ms/step - loss: 1.9444 - accuracy: 0.5463\n",
      "Epoch 82/250\n",
      "917/917 [==============================] - 159s 173ms/step - loss: 1.9395 - accuracy: 0.5480\n",
      "Epoch 83/250\n",
      "917/917 [==============================] - 158s 173ms/step - loss: 1.9352 - accuracy: 0.5485\n",
      "Epoch 84/250\n",
      "917/917 [==============================] - 159s 174ms/step - loss: 1.9293 - accuracy: 0.5488\n",
      "Epoch 85/250\n",
      "917/917 [==============================] - 159s 174ms/step - loss: 1.9182 - accuracy: 0.5521\n",
      "Epoch 86/250\n",
      "917/917 [==============================] - 159s 174ms/step - loss: 1.9125 - accuracy: 0.5538\n",
      "Epoch 87/250\n",
      "917/917 [==============================] - 162s 177ms/step - loss: 1.9058 - accuracy: 0.5542\n",
      "Epoch 88/250\n",
      "917/917 [==============================] - 162s 177ms/step - loss: 1.8959 - accuracy: 0.5567\n",
      "Epoch 89/250\n",
      "917/917 [==============================] - 163s 177ms/step - loss: 1.8923 - accuracy: 0.5578\n",
      "Epoch 90/250\n",
      "917/917 [==============================] - 163s 178ms/step - loss: 1.8855 - accuracy: 0.5581\n",
      "Epoch 91/250\n",
      "917/917 [==============================] - 163s 177ms/step - loss: 1.8905 - accuracy: 0.5578\n",
      "Epoch 92/250\n",
      "917/917 [==============================] - 162s 177ms/step - loss: 1.8770 - accuracy: 0.5611\n",
      "Epoch 93/250\n",
      "917/917 [==============================] - 163s 178ms/step - loss: 1.8644 - accuracy: 0.5632\n",
      "Epoch 94/250\n",
      "917/917 [==============================] - 163s 178ms/step - loss: 1.8640 - accuracy: 0.5630\n",
      "Epoch 95/250\n",
      "917/917 [==============================] - 164s 178ms/step - loss: 1.8578 - accuracy: 0.5639\n",
      "Epoch 96/250\n",
      "917/917 [==============================] - 165s 180ms/step - loss: 1.8477 - accuracy: 0.5664\n",
      "Epoch 97/250\n",
      "917/917 [==============================] - 166s 181ms/step - loss: 1.8459 - accuracy: 0.5655\n",
      "Epoch 98/250\n",
      "917/917 [==============================] - 167s 183ms/step - loss: 1.8443 - accuracy: 0.5667\n",
      "Epoch 99/250\n",
      "917/917 [==============================] - 169s 184ms/step - loss: 1.8319 - accuracy: 0.5698\n",
      "Epoch 100/250\n",
      "917/917 [==============================] - 170s 185ms/step - loss: 1.8236 - accuracy: 0.5716\n",
      "Epoch 101/250\n",
      "917/917 [==============================] - 171s 186ms/step - loss: 1.8226 - accuracy: 0.5712\n",
      "Epoch 102/250\n",
      "917/917 [==============================] - 171s 187ms/step - loss: 1.8302 - accuracy: 0.5689\n",
      "Epoch 103/250\n",
      "917/917 [==============================] - 170s 186ms/step - loss: 1.8056 - accuracy: 0.5741\n",
      "Epoch 104/250\n",
      "917/917 [==============================] - 170s 186ms/step - loss: 1.8050 - accuracy: 0.5750\n",
      "Epoch 105/250\n",
      "917/917 [==============================] - 172s 188ms/step - loss: 1.7996 - accuracy: 0.5761\n",
      "Epoch 106/250\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 1.8051 - accuracy: 0.5745\n",
      "Epoch 107/250\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 1.7913 - accuracy: 0.5778\n",
      "Epoch 108/250\n",
      "917/917 [==============================] - 174s 190ms/step - loss: 1.7804 - accuracy: 0.5800\n",
      "Epoch 109/250\n",
      "917/917 [==============================] - 173s 188ms/step - loss: 1.7770 - accuracy: 0.5797\n",
      "Epoch 110/250\n",
      "917/917 [==============================] - 175s 191ms/step - loss: 1.7769 - accuracy: 0.5802\n",
      "Epoch 111/250\n",
      "917/917 [==============================] - 176s 192ms/step - loss: 1.7703 - accuracy: 0.5815\n",
      "Epoch 112/250\n",
      "917/917 [==============================] - 177s 193ms/step - loss: 1.7620 - accuracy: 0.5837\n",
      "Epoch 113/250\n",
      "917/917 [==============================] - 176s 192ms/step - loss: 1.7581 - accuracy: 0.5851\n",
      "Epoch 114/250\n",
      "917/917 [==============================] - 176s 192ms/step - loss: 1.7591 - accuracy: 0.5841\n",
      "Epoch 115/250\n",
      "917/917 [==============================] - 179s 195ms/step - loss: 1.7591 - accuracy: 0.5842\n",
      "Epoch 116/250\n",
      "917/917 [==============================] - 180s 196ms/step - loss: 1.7375 - accuracy: 0.5883\n",
      "Epoch 117/250\n",
      "917/917 [==============================] - 181s 198ms/step - loss: 1.7387 - accuracy: 0.5886\n",
      "Epoch 118/250\n",
      "917/917 [==============================] - 182s 199ms/step - loss: 1.7349 - accuracy: 0.5893\n",
      "Epoch 119/250\n",
      "917/917 [==============================] - 180s 196ms/step - loss: 1.7256 - accuracy: 0.5919\n",
      "Epoch 120/250\n",
      "917/917 [==============================] - 182s 198ms/step - loss: 1.7274 - accuracy: 0.5901\n",
      "Epoch 121/250\n",
      "917/917 [==============================] - 182s 199ms/step - loss: 1.7225 - accuracy: 0.5908\n",
      "Epoch 122/250\n",
      "917/917 [==============================] - 183s 200ms/step - loss: 1.7073 - accuracy: 0.5964\n",
      "Epoch 123/250\n",
      "917/917 [==============================] - 183s 200ms/step - loss: 1.7122 - accuracy: 0.5945\n",
      "Epoch 124/250\n",
      "917/917 [==============================] - 183s 200ms/step - loss: 1.7085 - accuracy: 0.5946\n",
      "Epoch 125/250\n",
      "917/917 [==============================] - 184s 201ms/step - loss: 1.6929 - accuracy: 0.5975\n",
      "Epoch 126/250\n",
      "917/917 [==============================] - 185s 202ms/step - loss: 1.6984 - accuracy: 0.5971\n",
      "Epoch 127/250\n",
      "917/917 [==============================] - 185s 202ms/step - loss: 1.6965 - accuracy: 0.5964\n",
      "Epoch 128/250\n",
      "917/917 [==============================] - 186s 202ms/step - loss: 1.6835 - accuracy: 0.5998\n",
      "Epoch 129/250\n",
      "917/917 [==============================] - 185s 202ms/step - loss: 1.6775 - accuracy: 0.6008\n",
      "Epoch 130/250\n",
      "917/917 [==============================] - 187s 204ms/step - loss: 1.6865 - accuracy: 0.5979\n",
      "Epoch 131/250\n",
      "917/917 [==============================] - 187s 204ms/step - loss: 1.6807 - accuracy: 0.5993\n",
      "Epoch 132/250\n",
      "917/917 [==============================] - 188s 205ms/step - loss: 1.6710 - accuracy: 0.6016\n",
      "Epoch 133/250\n",
      "917/917 [==============================] - 188s 205ms/step - loss: 1.6587 - accuracy: 0.6050\n",
      "Epoch 134/250\n",
      "917/917 [==============================] - 186s 203ms/step - loss: 1.6632 - accuracy: 0.6027\n",
      "Epoch 135/250\n",
      "917/917 [==============================] - 190s 207ms/step - loss: 1.6577 - accuracy: 0.6046\n",
      "Epoch 136/250\n",
      "917/917 [==============================] - 191s 209ms/step - loss: 1.6595 - accuracy: 0.6044\n",
      "Epoch 137/250\n",
      "917/917 [==============================] - 191s 209ms/step - loss: 1.6473 - accuracy: 0.6069\n",
      "Epoch 138/250\n",
      "917/917 [==============================] - 192s 209ms/step - loss: 1.6375 - accuracy: 0.6093\n",
      "Epoch 139/250\n",
      "917/917 [==============================] - 191s 209ms/step - loss: 1.6367 - accuracy: 0.6085\n",
      "Epoch 140/250\n",
      "917/917 [==============================] - 194s 212ms/step - loss: 1.6413 - accuracy: 0.6088\n",
      "Epoch 141/250\n",
      "917/917 [==============================] - 194s 211ms/step - loss: 1.6360 - accuracy: 0.6095\n",
      "Epoch 142/250\n",
      "917/917 [==============================] - 195s 212ms/step - loss: 1.6162 - accuracy: 0.6144\n",
      "Epoch 143/250\n",
      "917/917 [==============================] - 194s 212ms/step - loss: 1.6224 - accuracy: 0.6117\n",
      "Epoch 144/250\n",
      "917/917 [==============================] - 193s 211ms/step - loss: 1.6227 - accuracy: 0.6117\n",
      "Epoch 145/250\n",
      "917/917 [==============================] - 195s 212ms/step - loss: 1.6056 - accuracy: 0.6145\n",
      "Epoch 146/250\n",
      "917/917 [==============================] - 195s 212ms/step - loss: 1.6104 - accuracy: 0.6130\n",
      "Epoch 147/250\n",
      "917/917 [==============================] - 196s 213ms/step - loss: 1.6077 - accuracy: 0.6138\n",
      "Epoch 148/250\n",
      "917/917 [==============================] - 195s 213ms/step - loss: 1.5994 - accuracy: 0.6176\n",
      "Epoch 149/250\n",
      "917/917 [==============================] - 197s 214ms/step - loss: 1.5965 - accuracy: 0.6163\n",
      "Epoch 150/250\n",
      "917/917 [==============================] - 198s 216ms/step - loss: 1.5977 - accuracy: 0.6159\n",
      "Epoch 151/250\n",
      "917/917 [==============================] - 199s 217ms/step - loss: 1.5884 - accuracy: 0.6184\n",
      "Epoch 152/250\n",
      "917/917 [==============================] - 199s 217ms/step - loss: 1.5768 - accuracy: 0.6219\n",
      "Epoch 153/250\n",
      "917/917 [==============================] - 198s 216ms/step - loss: 1.5804 - accuracy: 0.6198\n",
      "Epoch 154/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917/917 [==============================] - 201s 219ms/step - loss: 1.5828 - accuracy: 0.6200\n",
      "Epoch 155/250\n",
      "917/917 [==============================] - 204s 222ms/step - loss: 1.5683 - accuracy: 0.6242\n",
      "Epoch 156/250\n",
      "917/917 [==============================] - 204s 223ms/step - loss: 1.5703 - accuracy: 0.6223\n",
      "Epoch 157/250\n",
      "917/917 [==============================] - 203s 221ms/step - loss: 1.5676 - accuracy: 0.6227\n",
      "Epoch 158/250\n",
      "917/917 [==============================] - 205s 223ms/step - loss: 1.5506 - accuracy: 0.6261\n",
      "Epoch 159/250\n",
      "917/917 [==============================] - 204s 223ms/step - loss: 1.5511 - accuracy: 0.6264\n",
      "Epoch 160/250\n",
      "917/917 [==============================] - 206s 224ms/step - loss: 1.5558 - accuracy: 0.6247\n",
      "Epoch 161/250\n",
      "917/917 [==============================] - 206s 225ms/step - loss: 1.5502 - accuracy: 0.6271\n",
      "Epoch 162/250\n",
      "917/917 [==============================] - 207s 225ms/step - loss: 1.5424 - accuracy: 0.6285\n",
      "Epoch 163/250\n",
      "917/917 [==============================] - 208s 227ms/step - loss: 1.5417 - accuracy: 0.6282\n",
      "Epoch 164/250\n",
      "917/917 [==============================] - 208s 227ms/step - loss: 1.5384 - accuracy: 0.6297\n",
      "Epoch 165/250\n",
      "917/917 [==============================] - 209s 228ms/step - loss: 1.5492 - accuracy: 0.6257\n",
      "Epoch 166/250\n",
      "917/917 [==============================] - 208s 227ms/step - loss: 1.5348 - accuracy: 0.6308\n",
      "Epoch 167/250\n",
      "917/917 [==============================] - 210s 229ms/step - loss: 1.5290 - accuracy: 0.6315\n",
      "Epoch 168/250\n",
      "917/917 [==============================] - 210s 229ms/step - loss: 1.5189 - accuracy: 0.6330\n",
      "Epoch 169/250\n",
      "917/917 [==============================] - 211s 231ms/step - loss: 1.5229 - accuracy: 0.6309\n",
      "Epoch 170/250\n",
      "917/917 [==============================] - 210s 229ms/step - loss: 1.5152 - accuracy: 0.6333\n",
      "Epoch 171/250\n",
      "917/917 [==============================] - 213s 232ms/step - loss: 1.5095 - accuracy: 0.6363\n",
      "Epoch 172/250\n",
      "917/917 [==============================] - 213s 232ms/step - loss: 1.5151 - accuracy: 0.6329\n",
      "Epoch 173/250\n",
      "917/917 [==============================] - 214s 234ms/step - loss: 1.5180 - accuracy: 0.6319\n",
      "Epoch 174/250\n",
      "917/917 [==============================] - 214s 233ms/step - loss: 1.4914 - accuracy: 0.6395\n",
      "Epoch 175/250\n",
      "917/917 [==============================] - 213s 233ms/step - loss: 1.4869 - accuracy: 0.6416\n",
      "Epoch 176/250\n",
      "917/917 [==============================] - 214s 234ms/step - loss: 1.4863 - accuracy: 0.6400\n",
      "Epoch 177/250\n",
      "917/917 [==============================] - 216s 235ms/step - loss: 1.4843 - accuracy: 0.6416\n",
      "Epoch 178/250\n",
      "917/917 [==============================] - 216s 235ms/step - loss: 1.4970 - accuracy: 0.6374\n",
      "Epoch 179/250\n",
      "917/917 [==============================] - 216s 236ms/step - loss: 1.5061 - accuracy: 0.6344\n",
      "Epoch 180/250\n",
      "917/917 [==============================] - 217s 237ms/step - loss: 1.4735 - accuracy: 0.6427\n",
      "Epoch 181/250\n",
      "917/917 [==============================] - 218s 238ms/step - loss: 1.4844 - accuracy: 0.6401\n",
      "Epoch 182/250\n",
      "917/917 [==============================] - 219s 239ms/step - loss: 1.4806 - accuracy: 0.6409\n",
      "Epoch 183/250\n",
      "917/917 [==============================] - 219s 239ms/step - loss: 1.4870 - accuracy: 0.6396\n",
      "Epoch 184/250\n",
      "917/917 [==============================] - 221s 242ms/step - loss: 1.4776 - accuracy: 0.6418\n",
      "Epoch 185/250\n",
      "917/917 [==============================] - 222s 242ms/step - loss: 1.4700 - accuracy: 0.6441\n",
      "Epoch 186/250\n",
      "917/917 [==============================] - 222s 243ms/step - loss: 1.4722 - accuracy: 0.6417\n",
      "Epoch 187/250\n",
      "917/917 [==============================] - 223s 243ms/step - loss: 1.4655 - accuracy: 0.6450\n",
      "Epoch 188/250\n",
      "917/917 [==============================] - 225s 245ms/step - loss: 1.4645 - accuracy: 0.6439\n",
      "Epoch 189/250\n",
      "917/917 [==============================] - 224s 245ms/step - loss: 1.4696 - accuracy: 0.6423\n",
      "Epoch 190/250\n",
      "917/917 [==============================] - 227s 248ms/step - loss: 1.4437 - accuracy: 0.6491\n",
      "Epoch 191/250\n",
      "917/917 [==============================] - 226s 246ms/step - loss: 1.4573 - accuracy: 0.6458\n",
      "Epoch 192/250\n",
      "917/917 [==============================] - 227s 248ms/step - loss: 1.4729 - accuracy: 0.6413\n",
      "Epoch 193/250\n",
      "917/917 [==============================] - 228s 248ms/step - loss: 1.4455 - accuracy: 0.6490\n",
      "Epoch 194/250\n",
      "917/917 [==============================] - 229s 249ms/step - loss: 1.4547 - accuracy: 0.6458\n",
      "Epoch 195/250\n",
      "917/917 [==============================] - 229s 250ms/step - loss: 1.4338 - accuracy: 0.6516\n",
      "Epoch 196/250\n",
      "917/917 [==============================] - 231s 252ms/step - loss: 1.4335 - accuracy: 0.6513\n",
      "Epoch 197/250\n",
      "917/917 [==============================] - 231s 251ms/step - loss: 1.4428 - accuracy: 0.6480\n",
      "Epoch 198/250\n",
      "917/917 [==============================] - 232s 253ms/step - loss: 1.4335 - accuracy: 0.6493\n",
      "Epoch 199/250\n",
      "917/917 [==============================] - 231s 252ms/step - loss: 1.4167 - accuracy: 0.6541\n",
      "Epoch 200/250\n",
      "917/917 [==============================] - 235s 256ms/step - loss: 1.4283 - accuracy: 0.6517\n",
      "Epoch 201/250\n",
      "917/917 [==============================] - 234s 255ms/step - loss: 1.4303 - accuracy: 0.6511\n",
      "Epoch 202/250\n",
      "917/917 [==============================] - 234s 255ms/step - loss: 1.4147 - accuracy: 0.6545\n",
      "Epoch 203/250\n",
      "917/917 [==============================] - 234s 255ms/step - loss: 1.4166 - accuracy: 0.6532\n",
      "Epoch 204/250\n",
      "917/917 [==============================] - 236s 257ms/step - loss: 1.4341 - accuracy: 0.6504\n",
      "Epoch 205/250\n",
      "917/917 [==============================] - 237s 258ms/step - loss: 1.4161 - accuracy: 0.6535\n",
      "Epoch 206/250\n",
      "917/917 [==============================] - 236s 257ms/step - loss: 1.3969 - accuracy: 0.6580\n",
      "Epoch 207/250\n",
      "917/917 [==============================] - 234s 255ms/step - loss: 1.4123 - accuracy: 0.6531\n",
      "Epoch 208/250\n",
      "917/917 [==============================] - 235s 257ms/step - loss: 1.4064 - accuracy: 0.6555\n",
      "Epoch 209/250\n",
      "917/917 [==============================] - 237s 259ms/step - loss: 1.3954 - accuracy: 0.6588\n",
      "Epoch 210/250\n",
      "917/917 [==============================] - 238s 260ms/step - loss: 1.4034 - accuracy: 0.6553\n",
      "Epoch 211/250\n",
      "917/917 [==============================] - 237s 258ms/step - loss: 1.4013 - accuracy: 0.6577\n",
      "Epoch 212/250\n",
      "917/917 [==============================] - 241s 262ms/step - loss: 1.3946 - accuracy: 0.6582\n",
      "Epoch 213/250\n",
      "917/917 [==============================] - 240s 262ms/step - loss: 1.3877 - accuracy: 0.6605\n",
      "Epoch 214/250\n",
      "917/917 [==============================] - 242s 264ms/step - loss: 1.4173 - accuracy: 0.6536\n",
      "Epoch 215/250\n",
      "917/917 [==============================] - 239s 260ms/step - loss: 1.4143 - accuracy: 0.6526\n",
      "Epoch 216/250\n",
      "917/917 [==============================] - 241s 263ms/step - loss: 1.3660 - accuracy: 0.6666\n",
      "Epoch 217/250\n",
      "917/917 [==============================] - 240s 262ms/step - loss: 1.3644 - accuracy: 0.6656\n",
      "Epoch 218/250\n",
      "917/917 [==============================] - 243s 265ms/step - loss: 1.3836 - accuracy: 0.6613\n",
      "Epoch 219/250\n",
      "917/917 [==============================] - 244s 267ms/step - loss: 1.3937 - accuracy: 0.6578\n",
      "Epoch 220/250\n",
      "917/917 [==============================] - 243s 265ms/step - loss: 1.3821 - accuracy: 0.6611\n",
      "Epoch 221/250\n",
      "917/917 [==============================] - 245s 268ms/step - loss: 1.3669 - accuracy: 0.6649\n",
      "Epoch 222/250\n",
      "917/917 [==============================] - 250s 272ms/step - loss: 1.3604 - accuracy: 0.6653\n",
      "Epoch 223/250\n",
      "917/917 [==============================] - 247s 269ms/step - loss: 1.3561 - accuracy: 0.6679\n",
      "Epoch 224/250\n",
      "917/917 [==============================] - 248s 270ms/step - loss: 1.3760 - accuracy: 0.6627\n",
      "Epoch 225/250\n",
      "917/917 [==============================] - 249s 271ms/step - loss: 1.3764 - accuracy: 0.6624\n",
      "Epoch 226/250\n",
      "917/917 [==============================] - 249s 272ms/step - loss: 1.3648 - accuracy: 0.6660\n",
      "Epoch 227/250\n",
      "917/917 [==============================] - 249s 271ms/step - loss: 1.3613 - accuracy: 0.6640\n",
      "Epoch 228/250\n",
      "917/917 [==============================] - 249s 271ms/step - loss: 1.3549 - accuracy: 0.6679\n",
      "Epoch 229/250\n",
      "917/917 [==============================] - 250s 273ms/step - loss: 1.3495 - accuracy: 0.6681\n",
      "Epoch 230/250\n",
      "917/917 [==============================] - 251s 274ms/step - loss: 1.3694 - accuracy: 0.6618\n",
      "Epoch 231/250\n",
      "917/917 [==============================] - 252s 275ms/step - loss: 1.3466 - accuracy: 0.6687\n",
      "Epoch 232/250\n",
      "917/917 [==============================] - 253s 276ms/step - loss: 1.3309 - accuracy: 0.6734\n",
      "Epoch 233/250\n",
      "917/917 [==============================] - 255s 278ms/step - loss: 1.3423 - accuracy: 0.6706\n",
      "Epoch 234/250\n",
      "917/917 [==============================] - 257s 280ms/step - loss: 1.3629 - accuracy: 0.6652\n",
      "Epoch 235/250\n",
      "917/917 [==============================] - 258s 281ms/step - loss: 1.3301 - accuracy: 0.6722\n",
      "Epoch 236/250\n",
      "917/917 [==============================] - 257s 280ms/step - loss: 1.3415 - accuracy: 0.6689\n",
      "Epoch 237/250\n",
      "917/917 [==============================] - 259s 282ms/step - loss: 1.3461 - accuracy: 0.6677\n",
      "Epoch 238/250\n",
      "917/917 [==============================] - 261s 284ms/step - loss: 1.3324 - accuracy: 0.6710\n",
      "Epoch 239/250\n",
      "917/917 [==============================] - 262s 286ms/step - loss: 1.3382 - accuracy: 0.6708\n",
      "Epoch 240/250\n",
      "917/917 [==============================] - 267s 291ms/step - loss: 1.3238 - accuracy: 0.6735\n",
      "Epoch 241/250\n",
      "917/917 [==============================] - 274s 299ms/step - loss: 1.3254 - accuracy: 0.6730\n",
      "Epoch 242/250\n",
      "917/917 [==============================] - 261s 285ms/step - loss: 1.3233 - accuracy: 0.6727\n",
      "Epoch 243/250\n",
      "917/917 [==============================] - 262s 286ms/step - loss: 1.3333 - accuracy: 0.6701\n",
      "Epoch 244/250\n",
      "917/917 [==============================] - 263s 287ms/step - loss: 1.3328 - accuracy: 0.6737\n",
      "Epoch 245/250\n",
      "917/917 [==============================] - 263s 287ms/step - loss: 1.3219 - accuracy: 0.6720\n",
      "Epoch 246/250\n",
      "917/917 [==============================] - 264s 288ms/step - loss: 1.3210 - accuracy: 0.6733\n",
      "Epoch 247/250\n",
      "917/917 [==============================] - 264s 288ms/step - loss: 1.3218 - accuracy: 0.6729\n",
      "Epoch 248/250\n",
      "917/917 [==============================] - 264s 287ms/step - loss: 1.3116 - accuracy: 0.6764\n",
      "Epoch 249/250\n",
      "917/917 [==============================] - 267s 292ms/step - loss: 1.3024 - accuracy: 0.6769\n",
      "Epoch 250/250\n",
      "917/917 [==============================] - 269s 294ms/step - loss: 1.3308 - accuracy: 0.6719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b752c5d90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61715b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('modelx350.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4c327",
   "metadata": {},
   "source": [
    "### Use Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "772e7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load cleaned text sequences\n",
    "in_filename = 'republic_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5eeac",
   "metadata": {},
   "source": [
    "Нам нужен текст, чтобы мы могли выбрать исходную последовательность в качестве входных данных для модели для создания новой последовательности текста.\n",
    "\n",
    "Модель потребует 50 слов в качестве входных данных.\n",
    "\n",
    "Позже нам нужно будет указать ожидаемую длину ввода. Мы можем определить это по входным последовательностям, вычислив длину одной строки загруженных данных и вычтя 1 для ожидаемого выходного слова, которое также находится в той же строке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a6bb792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = len(lines[0].split()) - 1\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99a8a146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            366200    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7324)              739724    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,256,824\n",
      "Trainable params: 1,256,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x20ba4232910>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import models    \n",
    "\n",
    "# load the model\n",
    "model = models.load_model('modelx350.h5')\n",
    "print(model.summary())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8b15b",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af520e5a",
   "metadata": {},
   "source": [
    "Первым шагом в генерации текста является подготовка начального ввода.\n",
    "\n",
    "Для этого мы выберем случайную строку текста из входного текста. После выбора мы распечатаем его, чтобы иметь некоторое представление о том, что было использовано."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26ac5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pickle import dump\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c18c0cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replied for i suppose that you mean to exclude mere uninstructed courage such as that of a wild beast or of a in your opinion is not the courage which the law ordains and ought to have another name most certainly then i may infer courage to be such as you\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee45736",
   "metadata": {},
   "source": [
    "Далее мы можем генерировать новые слова по одному.\n",
    "\n",
    "Во-первых, начальный текст должен быть закодирован в целые числа с использованием того же токенизатора, который мы использовали при обучении модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "065df515",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fb05d",
   "metadata": {},
   "source": [
    "Модель может предсказать следующее слово напрямую, вызвав model.predict_classes(), которая вернет индекс слова с наибольшей вероятностью.\n",
    "\n",
    "Синтакис устарел (фукция удалена) с версии tansorflow 2.6, замена:\n",
    "\n",
    "#predict_x=model.predict(X_test) \n",
    "#classes_x=np.argmax(predict_x,axis=1)\n",
    "\n",
    "#y_predict = np.argmax(model.predict(x_test), axis=-1)\n",
    "\n",
    "#predictions = (model.predict(x_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a522343b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            366200    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7324)              739724    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,256,824\n",
      "Trainable params: 1,256,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for each word\n",
    "# yhat = model.predict_classes(encoded, verbose=0)\n",
    "#yhat=model.predict(encoded)\n",
    "#classes_x=np.argmax(yhat,axis=1)\n",
    "\n",
    "#y_predict = np.argmax(model.predict(x_test), axis=-1)\n",
    "\n",
    "#yhat = (model.predict(encoded) > 0.5).astype(\"int32\")\n",
    "\n",
    "#predictions = np.argmax(model.predict(encoded),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b4fce4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yhat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5248\\2303930669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mout_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mout_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'yhat' is not defined"
     ]
    }
   ],
   "source": [
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        out_word = word\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "384cb57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        #yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        yhat=model.predict(encoded)\n",
    "        classes_x=np.argmax(yhat,axis=1)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f0d4cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5248\\3585507110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# generate new text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgenerated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5248\\2117734533.py\u001b[0m in \u001b[0;36mgenerate_seq\u001b[1;34m(model, tokenizer, seq_length, seed_text, n_words)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mout_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 \u001b[0mout_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dcb4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
